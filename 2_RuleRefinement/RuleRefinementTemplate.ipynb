{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd66c5a7",
   "metadata": {},
   "source": [
    "## Rule Refinement Template Notebook:\n",
    "\n",
    "In this notebook, it is assumed that data has been mined from twitter and is present in a JSON format, in a local file.\n",
    "\n",
    "This notebook will separate all tweets by tag, and then run sub-section analysis on each of the tag dataframes.\n",
    "\n",
    "Users must manually look through the tweets (sadly), and classify what \"good\" and \"bad\" tweets are. \n",
    "\n",
    "Using simple data tidying, and intuitive methods (set difference between tokenized strings of \"good and bad\" tweets), we attempt to provide information to the user to properly refine their rules, and evaluate recent adjustments to their rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "239409f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load our custom library. Ouputs files in local directory automatically.\n",
    "from mods.dfprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetDF = generatedataframe(\"./data/runSept5_rule1-4.twts\",5000)\n",
    "\n",
    "def cleanDF(tDF):\n",
    "    tDF.drop_duplicates(subset=[\"text\"],inplace=True)\n",
    "    tDF['text'] = tDF[\"text\"].apply(lambda s: s.lower())\n",
    "    tDF['text'] = tDF[\"text\"].apply(lambda s: eliminate_slang_strings(s,screenWords))\n",
    "    tDF['text'] = tDF[\"text\"].apply(lambda s: remove_punct(s))\n",
    "    tDF['text'] = tDF[\"text\"].apply(lambda s: remove_noneng_chars(s))\n",
    "    return tDF\n",
    "\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b8f082bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5541 entries, 0 to 5540\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   tweetid        5541 non-null   int64 \n",
      " 1   text           5541 non-null   object\n",
      " 2   created_at     5541 non-null   object\n",
      " 3   tagid          5541 non-null   int64 \n",
      " 4   tag            5541 non-null   object\n",
      " 5   userid         5541 non-null   int64 \n",
      " 6   username       5541 non-null   object\n",
      " 7   rtcount        5541 non-null   int16 \n",
      " 8   repcount       5541 non-null   int16 \n",
      " 9   likecount      5541 non-null   int16 \n",
      " 10  qtcount        5541 non-null   int16 \n",
      " 11  tweet_type     5541 non-null   object\n",
      " 12  ref_tweetid    5541 non-null   int64 \n",
      " 13  ref_authorid   5541 non-null   int64 \n",
      " 14  ref_rtcount    5541 non-null   int16 \n",
      " 15  ref_repcount   5541 non-null   int16 \n",
      " 16  ref_likecount  5541 non-null   int16 \n",
      " 17  ref_qtcount    5541 non-null   int16 \n",
      "dtypes: int16(8), int64(5), object(5)\n",
      "memory usage: 519.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#Load the Dataframe: enter path\n",
    "tweetDF = generatedataframe(\"./data/runSept5_rule1-4.twts\",5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44605c0",
   "metadata": {},
   "source": [
    "For the next step, we need to clean the \"text\" field of the tweetDF dataframe. We eliminate the following:\n",
    "\n",
    "1) Removal of duplicate text rows (which occur when a user edits their tweet, or a spam account repeats itself many times).\n",
    "\n",
    "2) Apply lower-case to all text (helps simplify our tokenizing).\n",
    "\n",
    "3) Clip out emoticons and weird characters (English characters only!)\n",
    "\n",
    "4) Filter for slang or bullshit terms ( \"fr fr\", \"i got u fam\" \"trolololololo\" \"ya'll!!\", etc...).\n",
    "\n",
    "Lets get Started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82ad2497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect duplicate text rows, and cut down tweetDF\n",
    "tweetDF.drop_duplicates(subset=[\"text\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d1b87386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next make everything lowercase in the text column\n",
    "tweetDF['text'] = tweetDF[\"text\"].apply(lambda s: s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b12e7f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets generate our word removal list\n",
    "#remember to apply after lowercase function!\n",
    "screenWords = [\"celebs\",\"frfr\",\"fr fr\",\"lulz\",\"rofl\",\n",
    "              \"roflmao\",\"lmao\",\"lol\",\"chuds\",\"yall\",\"y'all\",\n",
    "              \"dem\",\"demz\",\"hella\",\"cums\",\"onlyfans\",\"only fans\",\n",
    "              \"plz\",\"pls\",\"noob\",\"grindset\",\"vibe\",\"vibrations\",\n",
    "              \"gurl\",\"chill\",\"nft\",\"coom\",\"cringe\",\"based\",\"alpha\",\n",
    "               \"beta\",\"sigma\",\"mindset\",\"babe\",\"tpot\",\"flex\",\n",
    "               \"moon\",\"pumps\",\"apes\",\"celeb\",\"cuck\",\"cucked\",\n",
    "              \"smh\",\"goes hard\",\" stan \",\"jesus\",\"lord\",\" da \",\n",
    "               \"ass\",\"mfers\",\"mfer\",\"thicc\",\"nigga\",\"!!\",\"!!!\",\n",
    "              \"??\",\"???\",\"http://\",\"https://\",\"ya'll\"]\n",
    "\n",
    "tweetDF['text'] = tweetDF[\"text\"].apply(lambda s: eliminate_slang_strings(s,screenWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "43199a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(data):\n",
    "    replace = re.compile(\"[\"\n",
    "        \".\"\n",
    "        \"!\"\n",
    "        \"?\"\n",
    "        \"\\n\"\n",
    "        \"/\"               \n",
    "        \"\\\"\"\n",
    "        \",\"\n",
    "        \"}\"\n",
    "        \"{\"\n",
    "        \"[\"\n",
    "        \"]\"\n",
    "        \"<\"\n",
    "        \">\"\n",
    "        \"(\"\n",
    "        \")\"\n",
    "        \"+\"\n",
    "        \":\"\n",
    "        \";\"\n",
    "                    \"]+\", re.UNICODE)\n",
    "    return re.sub(replace,\"\",data)\n",
    "\n",
    "tweetDF['text'] = tweetDF[\"text\"].apply(lambda s: remove_punct(s))\n",
    "tweetDF['text'] = tweetDF[\"text\"].apply(lambda s: remove_noneng_chars(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2cb061c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetDF.drop_duplicates(subset=[\"text\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "36aafeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next, separate Dataframes based on tags. Get the tags, and call\n",
    "#our separator a number of times.\n",
    "tagList = (tweetDF.tag.unique()).tolist()\n",
    "\n",
    "#SelfAndID \n",
    "#WE have to add an index column, as to_json doesn't write when we orient=records\n",
    "saiDF = (tweetDF[tweetDF[\"tag\"] == \"SelfandID\"]).copy(deep=True).reset_index(drop=True).reset_index()\n",
    "#SearchTheVoid\n",
    "stvDF = (tweetDF[tweetDF[\"tag\"] == \"SearchTheVoid\"]).copy(deep=True).reset_index(drop=True).reset_index()\n",
    "#SocietalShift\n",
    "ssDF = (tweetDF[tweetDF[\"tag\"] == \"SocietalShift\"]).copy(deep=True).reset_index(drop=True).reset_index()\n",
    "#HealthySkepticism\n",
    "hsDF = (tweetDF[tweetDF[\"tag\"] == \"HealthySkepticism\"]).copy(deep=True).reset_index(drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3e87660f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 19)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0c0b3",
   "metadata": {},
   "source": [
    "### SelfAndID Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cf0e9",
   "metadata": {},
   "source": [
    "First, we export the row indicies and text column, to identify what is a good tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2b53f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "saiDF.loc[:,[\"index\",\"tweetid\",\"text\"]].to_json(\"./data/saiDF.json\",orient=\"records\",index=True,force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "37eed13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saiGoodIndices = [0,62,65,291,381,445,447,566,592,641,945,960,1144,1167]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd358db6",
   "metadata": {},
   "source": [
    "After going through almost 3000 tweets (ugh), I was able to extract about 30 \"good\" tweets, that mirror what I am looking for. The rest were spam, jibberish, or poor takes.\n",
    "\n",
    "A few other filters will also have to be devised (implemented above). Most tweets that have URL likes (http://t.co ...) are crap tweets.\n",
    "In addition, there is more spam than I imagined. There are many automated accounts that will post the same tweet, with a hashnumber appended to the end, to fool the twitter algorithm. Consider the following example, below:\n",
    "\n",
    "**Note:** Cutting out URL tweets reduced our tweet set by a factor of 2. Noticable improvement!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd7b03",
   "metadata": {},
   "source": [
    "So in addition to further screening, we need a function that measures string similarity (and apply a threshold test for all of our strings.\n",
    "\n",
    "Based on this Stack Overflow post: https://stackoverflow.com/questions/17388213/find-the-similarity-metric-between-two-strings\n",
    "\n",
    "The Levenshtein distance would be quite appropriate (strings don't have to be the same length, does not prefer prefix matches over other placed matches). However Comparing every string to every other strings is O(m n^2) complexity, which could end up cubic as Levenshtein recursive on string characters...\n",
    "\n",
    "A faster method would be to perhaps calculate the LD for each row, based on a reference string \"The Quick brown fox....\", and then do histogram binning on strings with similar distances. This is an approximation at best, and may have significant errors, however.\n",
    "\n",
    "I might implement this later, if the spam gets too bad. It might be easier just to add more constraints, to avoid going down this rabbit hole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa751d",
   "metadata": {},
   "source": [
    "Now it is time to build the Tokenizing Sets and Hash functions, to do counts and set difference operations. From this, I hope to gather information about how to adjust my rules to get fewer, more focused matches. Lets get started...\n",
    "\n",
    "**Remember:** You must complete all text cleaning before you identify \"good indicies\", else you risk referencing shifted/fictitious rows if you clean a second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f26a4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "saiGoodDict = {}\n",
    "saiBadDict = {}\n",
    "saiGoodSet = set()\n",
    "saiBadSet = set()\n",
    "\n",
    "#For each text string in our saiDF:\n",
    "#Separate out good rows from bad (two separate dataframes)\n",
    "#\n",
    "saiGoodSer = saiDF[\"text\"].iloc[saiGoodIndices]\n",
    "saiBadSer = saiDF[~saiDF.index.isin(saiGoodIndices)][\"text\"]\n",
    "\n",
    "\n",
    "#set, dictionary -> None (mutate arguments)\n",
    "def insert_hashandset(textSeries,wSet,wDict):\n",
    "    for text in textSeries:\n",
    "        sTokens = text.split(\" \")\n",
    "        #Lets screen out tokens that are <= 2 in length,\n",
    "        #Or are just punctuation or spaces\n",
    "        for i in range(0,len(sTokens)):\n",
    "            hold = sTokens[i]\n",
    "            if ((len(hold) > 3)):\n",
    "                wSet.add(hold)\n",
    "                if (hold in wDict):\n",
    "                    wDict[hold] = wDict[hold] + 1\n",
    "                else:\n",
    "                    wDict[hold] = 1\n",
    "    return\n",
    "        \n",
    "insert_hashandset(saiGoodSer,saiGoodSet,saiGoodDict)  \n",
    "insert_hashandset(saiBadSer,saiBadSet,saiBadDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d5a8ce6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saiGoodSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a2f0b866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4560"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saiBadSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "35b36074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueGoodWords = saiGoodSet.difference(saiBadSet)\n",
    "uniqueBadWords = saiBadSet.difference(saiGoodSet)\n",
    "intersectWords = saiGoodSet.intersection(saiBadSet)\n",
    "len(intersectWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c892b502",
   "metadata": {},
   "source": [
    "### Visualizing our Top Words:\n",
    "\n",
    "We need a fast way to rank our words by counts. Dictionaries are too primitive, and heaps/trees can be made to work on composite objects, but coding this takes time. When doing data analytics, the most important data structure is of course the DataFrame. A short column table will be made for each wordHash, which we plug+chug with our word sets. We then just do a row sort on the finished data structure. Easy enough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d6745dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33 entries, 0 to 32\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   word    33 non-null     object\n",
      " 1   count   33 non-null     int16 \n",
      "dtypes: int16(1), object(1)\n",
      "memory usage: 458.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "#making our saiGoodDF first:\n",
    "\n",
    "saiGoodDF = pd.DataFrame(columns=[\"word\",\"count\"])\n",
    "wordList = []\n",
    "countList = []\n",
    "for word in uniqueGoodWords:\n",
    "    wordList.append(word)\n",
    "    countList.append(saiGoodDict[word])\n",
    "\n",
    "saiGoodDF[\"word\"] = pd.Series(wordList)\n",
    "saiGoodDF[\"count\"] = pd.Series(countList,dtype=\"int16\")\n",
    "saiGoodDF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a81f25",
   "metadata": {},
   "source": [
    "And, looking at our saiGoodDF, all of our counts are one! \n",
    "\n",
    "It looks like all happy families are fairly unique. What about the unhappy ones? Can we find high counts of bad words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "49af39d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4377 entries, 0 to 4376\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   word    4377 non-null   object\n",
      " 1   count   4377 non-null   int16 \n",
      "dtypes: int16(1), object(1)\n",
      "memory usage: 42.9+ KB\n"
     ]
    }
   ],
   "source": [
    "saiBadDF = pd.DataFrame(columns=[\"word\",\"count\"])\n",
    "wordList = []\n",
    "countList = []\n",
    "for word in uniqueBadWords:\n",
    "    wordList.append(word)\n",
    "    countList.append(saiBadDict[word])\n",
    "\n",
    "saiBadDF[\"word\"] = pd.Series(wordList)\n",
    "saiBadDF[\"count\"] = pd.Series(countList,dtype=\"int16\")\n",
    "saiBadDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c2d87fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>dnj3989</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3672</th>\n",
       "      <td>said</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>there</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>personal</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3950</th>\n",
       "      <td>sort</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4309</th>\n",
       "      <td>ٱےـهـربَـ</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4059</th>\n",
       "      <td>نوننمشيay20ay20ٱےهےربَـ</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>didn'tك̷و̷د̶</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td≯خ̷ص̸م̴</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>best</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>know</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>real</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>fraud</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>better</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>isn't</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>loss</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>made</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4115</th>\n",
       "      <td>ك̷و̷د̶⁩</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>empire</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2843</th>\n",
       "      <td>ے⁦نون⁩-</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>⁦̸خ̷ص̸م̴⁩</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>education</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>thean</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>ay20</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>don't</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>think</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>want</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>it’s</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>back</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>through</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>need</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>really</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>then</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>some</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>should</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>today</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>right</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>before</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>that's</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>long</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>first</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>getting</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>these</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>after</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>here</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>can’t</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3406</th>\n",
       "      <td>something</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>home</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>those</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>you’re</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3785</th>\n",
       "      <td>hope</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282</th>\n",
       "      <td>sometimes</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2681</th>\n",
       "      <td>others</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>years</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036</th>\n",
       "      <td>remember</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>open</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>keep</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>come</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>world</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>days</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>show</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>break</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>other</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>twitter</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>were</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>point</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>proud</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3491</th>\n",
       "      <td>started</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3180</th>\n",
       "      <td>please</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>stay</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4330</th>\n",
       "      <td>could</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>stop</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>lose</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4151</th>\n",
       "      <td>year</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         word  count\n",
       "1572                  dnj3989    250\n",
       "3672                     said    137\n",
       "3918                    there    136\n",
       "2918                 personal    132\n",
       "3950                     sort    126\n",
       "4309                ٱےـهـربَـ    125\n",
       "4059  نوننمشيay20ay20ٱےهےربَـ    125\n",
       "139              didn'tك̷و̷د̶    125\n",
       "2290                  ̸خ̷ص̸م̴    125\n",
       "1599                     best    119\n",
       "1103                     know    117\n",
       "366                      real    114\n",
       "1418                    fraud    113\n",
       "556                    better    109\n",
       "81                      isn't    105\n",
       "3349                     loss    102\n",
       "330                      made    101\n",
       "4115                  ك̷و̷د̶⁩     99\n",
       "193                    empire     99\n",
       "2843                  ے⁦نون⁩-     99\n",
       "2328                ⁦̸خ̷ص̸م̴⁩     99\n",
       "2622                education     98\n",
       "4                        just     98\n",
       "120                     thean     97\n",
       "693                      ay20     97\n",
       "2889                    don't     91\n",
       "3365                    think     53\n",
       "2494                     want     49\n",
       "479                      it’s     42\n",
       "1818                     back     41\n",
       "3909                  through     35\n",
       "473                      need     32\n",
       "29                     really     30\n",
       "1360                     then     27\n",
       "3751                     some     26\n",
       "2307                   should     25\n",
       "470                     today     25\n",
       "1488                    right     24\n",
       "3024                   before     22\n",
       "4021                   that's     21\n",
       "1459                     long     21\n",
       "3741                    first     21\n",
       "2752                  getting     21\n",
       "2364                    these     21\n",
       "1964                    after     20\n",
       "1286                     here     20\n",
       "363                     can’t     20\n",
       "3406                something     19\n",
       "461                      home     19\n",
       "2596                    those     19\n",
       "2878                   you’re     18\n",
       "3785                     hope     18\n",
       "3282                sometimes     18\n",
       "2681                   others     17\n",
       "1867                    years     17\n",
       "3036                 remember     17\n",
       "1024                     open     17\n",
       "1785                     keep     17\n",
       "3249                     come     17\n",
       "2026                    world     17\n",
       "165                      days     16\n",
       "3043                     show     16\n",
       "255                     break     16\n",
       "179                     other     16\n",
       "849                   twitter     16\n",
       "3928                     were     15\n",
       "2964                    point     15\n",
       "2559                    proud     15\n",
       "3491                  started     15\n",
       "3180                   please     14\n",
       "1632                     stay     14\n",
       "4330                    could     14\n",
       "2569                     stop     14\n",
       "1331                     lose     14\n",
       "4151                     year     14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "saiBadDF.sort_values(ascending=False,axis=\"index\",by=\"count\",inplace=True)\n",
    "display(saiBadDF.head(75))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bd4b7",
   "metadata": {},
   "source": [
    "**Conclusion:** The following words need to be screened out from the search string: empire, personal, fraud, real, education. Also the word \"journey\" needs to be removed from search, which was highly correlated with clickbait and link posting.\n",
    "\n",
    "Will try to see if this improves our rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805af22",
   "metadata": {},
   "source": [
    "### SearchingtheVoid DataFrame Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3afaef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(724, 19)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stvDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "89d87ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export and select \"good indicies\"\n",
    "stvDF.loc[:,[\"index\",\"tweetid\",\"text\"]].to_json(\"./data/stvDF.json\",orient=\"records\",index=True,force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af8262",
   "metadata": {},
   "source": [
    "**Conclusion**: I found a whole three tweets that I would label as good. This is simply too few to do a set difference analysis. Almost every tweet in the set has the words \"help me\" - this leads to an incredible amount of spam, bullshit requests that fall on deaf ears, and the odd post that has undertones of suicidal ideation. \"Help me\" is getting removed from the rule value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0582a",
   "metadata": {},
   "source": [
    "### The Other Data Frames:\n",
    "\n",
    "Both HealthySkepticism and Societal Shift have <120 tweets. I skimmed both of them just to see if there were any terms (by eye) that were highly correlated with low quality tweets. Didn't find anything  \n",
    "\n",
    "¯\\_(ツ)_/¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ef008bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssDF.loc[:,[\"index\",\"tweetid\",\"text\"]].to_json(\"./data/ssDF.json\",orient=\"records\",index=True,force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2a63ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsDF.loc[:,[\"index\",\"tweetid\",\"text\"]].to_json(\"./data/hsDF.json\",orient=\"records\",index=True,force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbfb4be",
   "metadata": {},
   "source": [
    "### Final Result:\n",
    "\n",
    "After adjusting my rules, there was a significant drop-off in our stream rate. I end up with one match every 30 seconds!\n",
    "\n",
    "Hopefully spam/garbage will be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9507c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d656647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb62c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41bb384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41cc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae261e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a2187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ee4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d223f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
