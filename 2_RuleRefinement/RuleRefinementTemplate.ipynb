{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd66c5a7",
   "metadata": {},
   "source": [
    "## Rule Refinement Template Notebook:\n",
    "\n",
    "In this notebook, it is assumed that data has been mined from twitter and is present in a JSON format, in a local file.\n",
    "\n",
    "This notebook will separate all tweets by tag, and then run sub-section analysis on each of the tag dataframes.\n",
    "\n",
    "Users must manually look through the tweets (sadly), and classify what \"good\" and \"bad\" tweets are. \n",
    "\n",
    "Using simple data tidying, and intuitive methods (set difference between tokenized strings of \"good and bad\" tweets), we attempt to provide information to the user to properly refine their rules, and evaluate recent adjustments to their rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "239409f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load our custom library. Ouputs files in local directory automatically.\n",
    "from mods.dfprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b8f082bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5541 entries, 0 to 5540\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   tweetid        5541 non-null   int64 \n",
      " 1   text           5541 non-null   object\n",
      " 2   created_at     5541 non-null   object\n",
      " 3   tagid          5541 non-null   int64 \n",
      " 4   tag            5541 non-null   object\n",
      " 5   userid         5541 non-null   int64 \n",
      " 6   username       5541 non-null   object\n",
      " 7   rtcount        5541 non-null   int16 \n",
      " 8   repcount       5541 non-null   int16 \n",
      " 9   likecount      5541 non-null   int16 \n",
      " 10  qtcount        5541 non-null   int16 \n",
      " 11  tweet_type     5541 non-null   object\n",
      " 12  ref_tweetid    5541 non-null   int64 \n",
      " 13  ref_authorid   5541 non-null   int64 \n",
      " 14  ref_rtcount    5541 non-null   int16 \n",
      " 15  ref_repcount   5541 non-null   int16 \n",
      " 16  ref_likecount  5541 non-null   int16 \n",
      " 17  ref_qtcount    5541 non-null   int16 \n",
      "dtypes: int16(8), int64(5), object(5)\n",
      "memory usage: 519.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#Load the Dataframe: enter path\n",
    "tweetDF = generatedataframe(\"./data/runSept5_rule1-4.twts\",5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44605c0",
   "metadata": {},
   "source": [
    "For the next step, we need to clean the \"text\" field of the tweetDF dataframe. We eliminate the following:\n",
    "\n",
    "1) Removal of duplicate text rows (which occur when a user edits their tweet, or a spam account repeats itself many times).\n",
    "\n",
    "2) Apply lower-case to all text (helps simplify our tokenizing).\n",
    "\n",
    "3) Clip out emoticons and weird characters (English characters only!)\n",
    "\n",
    "4) Filter for slang or bullshit terms ( \"fr fr\", \"i got u fam\" \"trolololololo\" \"ya'll!!\", etc...).\n",
    "\n",
    "Lets get Started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82ad2497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect duplicate text rows, and cut down tweetDF\n",
    "tweetDF.drop_duplicates(subset=[\"text\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d1b87386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next make everything lowercase in the text column\n",
    "tweetDF['text'] = tweetDF[\"text\"].apply(lambda s: s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b12e7f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets generate our word removal list\n",
    "#remember to apply after lowercase function!\n",
    "screenWords = [\"celebs\",\"frfr\",\"fr fr\",\"lulz\",\"rofl\",\n",
    "              \"roflmao\",\"lmao\",\"lol\",\"chuds\",\"yall\",\"y'all\",\n",
    "              \"dem\",\"demz\",\"hella\",\"cums\",\"onlyfans\",\"only fans\",\n",
    "              \"plz\",\"pls\",\"noob\",\"grindset\",\"vibe\",\"vibrations\",\n",
    "              \"gurl\",\"chill\",\"nft\",\"coom\",\"cringe\",\"based\",\"alpha\",\n",
    "               \"beta\",\"sigma\",\"mindset\",\"babe\",\"tpot\",\"flex\",\n",
    "               \"moon\",\"pumps\",\"apes\",\"celeb\",\"cuck\",\"cucked\",\n",
    "              \"smh\",\"goes hard\",\" stan \",\"jesus\",\"lord\",\" da \",\n",
    "               \"ass\",\"mfers\",\"mfer\",\"thicc\",\"nigga\",\"!!\",\"!!!\",\n",
    "              \"??\",\"???\",\"http://\",\"https://\",\"ya'll\"]\n",
    "\n",
    "tweetDF['text'] = tweetDF[\"text\"].apply(lambda s: eliminate_slang_strings(s,screenWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6a7bbffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(data):\n",
    "    replace = re.compile(\"[\"\n",
    "        \".\"\n",
    "        \"!\"\n",
    "        \"?\"\n",
    "        \"\\n\"\n",
    "        \"/\"               \n",
    "        \"\\\"\"\n",
    "        \",\"\n",
    "        \"}\"\n",
    "        \"{\"\n",
    "        \"[\"\n",
    "        \"]\"\n",
    "        \"<\"\n",
    "        \">\"\n",
    "        \"(\"\n",
    "        \")\"\n",
    "        \"+\"\n",
    "        \":\"\n",
    "        \";\"\n",
    "                    \"]+\", re.UNICODE)\n",
    "    return re.sub(replace,\"\",data)\n",
    "\n",
    "tweetDF['text'] = tweetDF[\"text\"].apply(lambda s: remove_punct(s))\n",
    "tweetDF['text'] = tweetDF[\"text\"].apply(lambda s: remove_noneng_chars(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2cb061c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetDF.drop_duplicates(subset=[\"text\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "36aafeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next, separate Dataframes based on tags. Get the tags, and call\n",
    "#our separator a number of times.\n",
    "tagList = (tweetDF.tag.unique()).tolist()\n",
    "\n",
    "#SelfAndID \n",
    "#WE have to add an index column, as to_json doesn't write when we orient=records\n",
    "saiDF = (tweetDF[tweetDF[\"tag\"] == \"SelfandID\"]).copy(deep=True).reset_index(drop=True).reset_index()\n",
    "#SearchTheVoid\n",
    "stvDF = (tweetDF[tweetDF[\"tag\"] == \"SearchTheVoid\"]).copy(deep=True).reset_index(drop=True).reset_index()\n",
    "#SocietalShift\n",
    "ssDF = (tweetDF[tweetDF[\"tag\"] == \"SocietalShift\"]).copy(deep=True).reset_index(drop=True).reset_index()\n",
    "#HealthySkepticism\n",
    "hsDF = (tweetDF[tweetDF[\"tag\"] == \"HealthySkepticism\"]).copy(deep=True).reset_index(drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2837ffe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1172, 19)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saiDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0c0b3",
   "metadata": {},
   "source": [
    "### SelfAndID Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cf0e9",
   "metadata": {},
   "source": [
    "First, we export the row indicies and text column, to identify what is a good tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2b53f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "saiDF.loc[:,[\"index\",\"tweetid\",\"text\"]].to_json(\"./data/saiDF.json\",orient=\"records\",index=True,force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "37eed13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saiGoodIndices = [0,62,65,291,381,445,447,566,592,641,945,960,1144,1167]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03a976",
   "metadata": {},
   "source": [
    "After going through almost 3000 tweets (ugh), I was able to extract about 30 \"good\" tweets, that mirror what I am looking for. The rest were spam, jibberish, or poor takes.\n",
    "\n",
    "A few other filters will also have to be devised (implemented above). Most tweets that have URL likes (http://t.co ...) are crap tweets.\n",
    "In addition, there is more spam than I imagined. There are many automated accounts that will post the same tweet, with a hashnumber appended to the end, to fool the twitter algorithm. Consider the following example, below:\n",
    "\n",
    "**Note:** Cutting out URL tweets reduced our tweet set by a factor of 2. Noticable improvement!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a85297f",
   "metadata": {},
   "source": [
    "So in addition to further screening, we need a function that measures string similarity (and apply a threshold test for all of our strings.\n",
    "\n",
    "Based on this Stack Overflow post: https://stackoverflow.com/questions/17388213/find-the-similarity-metric-between-two-strings\n",
    "\n",
    "The Levenshtein distance would be quite appropriate (strings don't have to be the same length, does not prefer prefix matches over other placed matches). However Comparing every string to every other strings is O(m n^2) complexity, which could end up cubic as Levenshtein recursive on string characters...\n",
    "\n",
    "A faster method would be to perhaps calculate the LD for each row, based on a reference string \"The Quick brown fox....\", and then do histogram binning on strings with similar distances. This is an approximation at best, and may have significant errors, however.\n",
    "\n",
    "I might implement this later, if the spam gets too bad. It might be easier just to add more constraints, to avoid going down this rabbit hole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3fb57a",
   "metadata": {},
   "source": [
    "Now it is time to build the Tokenizing Sets and Hash functions, to do counts and set difference operations. From this, I hope to gather information about how to adjust my rules to get fewer, more focused matches. Lets get started...\n",
    "\n",
    "**Remember:** You must complete all text cleaning before you identify \"good indicies\", else you risk referencing shifted/fictitious rows if you clean a second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f26a4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "saiGoodDict = {}\n",
    "saiBadDict = {}\n",
    "saiGoodSet = set()\n",
    "saiBadSet = set()\n",
    "\n",
    "#For each text string in our saiDF:\n",
    "#Separate out good rows from bad (two separate dataframes)\n",
    "#\n",
    "saiGoodSer = saiDF[\"text\"].iloc[saiGoodIndices]\n",
    "saiBadSer = saiDF[~saiDF.index.isin(saiGoodIndices)][\"text\"]\n",
    "\n",
    "\n",
    "#set, dictionary -> None (mutate arguments)\n",
    "def insert_hashandset(textSeries,wSet,wDict):\n",
    "    for text in textSeries:\n",
    "        sTokens = text.split(\" \")\n",
    "        #Lets screen out tokens that are <= 2 in length,\n",
    "        #Or are just punctuation or spaces\n",
    "        for i in range(0,len(sTokens)):\n",
    "            hold = sTokens[i]\n",
    "            if ((len(hold) > 3)):\n",
    "                wSet.add(hold)\n",
    "                if (hold in wDict):\n",
    "                    wDict[hold] = wDict[hold] + 1\n",
    "                else:\n",
    "                    wDict[hold] = 1\n",
    "    return\n",
    "        \n",
    "insert_hashandset(saiGoodSer,saiGoodSet,saiGoodDict)  \n",
    "insert_hashandset(saiBadSer,saiBadSet,saiBadDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d5a8ce6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saiGoodSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a2f0b866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4560"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saiBadSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0bb1adeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueGoodWords = saiGoodSet.difference(saiBadSet)\n",
    "uniqueBadWords = saiBadSet.difference(saiGoodSet)\n",
    "intersectWords = saiGoodSet.intersection(saiBadSet)\n",
    "len(intersectWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "544834dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apply',\n",
       " 'consume',\n",
       " 'creating',\n",
       " 'cruising',\n",
       " 'deem',\n",
       " 'deeper',\n",
       " 'disagree',\n",
       " 'discernment',\n",
       " 'easymaintaining',\n",
       " 'edginess',\n",
       " 'establish',\n",
       " \"fandom's\",\n",
       " 'grateful',\n",
       " 'hurting',\n",
       " 'ideas',\n",
       " 'infinitely',\n",
       " 'method',\n",
       " \"people's\",\n",
       " 'people;',\n",
       " 'plot-twist:',\n",
       " 'production',\n",
       " 're-start',\n",
       " 'remove',\n",
       " 'resources',\n",
       " 'self-preservation',\n",
       " 'shedding',\n",
       " 'someb',\n",
       " 'soul-draining',\n",
       " 'surrounded',\n",
       " 'understanding',\n",
       " 'unemployed',\n",
       " 'valid',\n",
       " 'worthy'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueGoodWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6745dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what are our counts?\n",
    "for word in uniqueGoodWords:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877f78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af39d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d87fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aad884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1aec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121b4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ee4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d223f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
