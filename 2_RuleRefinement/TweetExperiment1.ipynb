{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ProposedRules.txt', 'testtweets.twts', 'testdatasets', 'rules1-4_trial1.twts']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./data\"))\n",
    "\n",
    "testFilePath = \"./data/testtweets.twts\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "\n",
    "In this notebook, I hope to load tweets from a file, parse them as json objects and do basic analytics on the said data.The tweets are in UTF-8, so cleaning and parsing the data may be challenging. Lets see how it goes...\n",
    "\n",
    "To begin, lets load the dataset. First we use the top 200 tweets in the file, as a simple test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>matching_rules</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': '1560703992908451840', 'text': 'RT @dav...</td>\n",
       "      <td>[{'id': '1560702401342017538', 'tag': 'Selfand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': '1560703995701669889', 'text': 'RT @Fox...</td>\n",
       "      <td>[{'id': '1560702401342017538', 'tag': 'Selfand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': '1560703992308457472', 'text': 'RT @hos...</td>\n",
       "      <td>[{'id': '1560702401342017538', 'tag': 'Selfand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'id': '1560703992736272384', 'text': 'In this...</td>\n",
       "      <td>[{'id': '1560702401342017538', 'tag': 'Selfand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'id': '1560703994040909827', 'text': 'RT @Tej...</td>\n",
       "      <td>[{'id': '1560702401342017538', 'tag': 'Selfand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  \\\n",
       "0  {'id': '1560703992908451840', 'text': 'RT @dav...   \n",
       "1  {'id': '1560703995701669889', 'text': 'RT @Fox...   \n",
       "2  {'id': '1560703992308457472', 'text': 'RT @hos...   \n",
       "3  {'id': '1560703992736272384', 'text': 'In this...   \n",
       "4  {'id': '1560703994040909827', 'text': 'RT @Tej...   \n",
       "\n",
       "                                      matching_rules  \n",
       "0  [{'id': '1560702401342017538', 'tag': 'Selfand...  \n",
       "1  [{'id': '1560702401342017538', 'tag': 'Selfand...  \n",
       "2  [{'id': '1560702401342017538', 'tag': 'Selfand...  \n",
       "3  [{'id': '1560702401342017538', 'tag': 'Selfand...  \n",
       "4  [{'id': '1560702401342017538', 'tag': 'Selfand...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTweets = pd.read_json(testFilePath,lines=True)\n",
    "print(dfTweets.shape)\n",
    "dfTweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ends up with dictionary objects in our entry cells, which is annoying. It looks like read_json() does not have parameters to stop this. \n",
    "\n",
    "A simple file read loop, with jsonparse() is probably a better approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetDF = pd.DataFrame(columns=[\"tweetid\", \"text\", \"tagid\",\"tag\"])\n",
    "##Note! On a rare occasion, two or more tags can match. THis currently chooses the first tag set\n",
    "##Information loss can occur.\n",
    "with open(testFilePath) as fp:\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        jsonObj = json.loads(line)\n",
    "        v1 = jsonObj[\"data\"][\"id\"]\n",
    "        v2 = jsonObj[\"data\"][\"text\"]\n",
    "        v3 = jsonObj['matching_rules'][0][\"id\"]\n",
    "        v4 = jsonObj['matching_rules'][0][\"tag\"]\n",
    "        tweetDF.loc[len(tweetDF.index)] = [v1,v2,v3,v4]    \n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "\n",
    "tweetDF[\"tweetid\"] = pd.to_numeric(tweetDF[\"tweetid\"])\n",
    "tweetDF[\"tagid\"] = pd.to_numeric(tweetDF[\"tagid\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "\n",
      "tweetid    0\n",
      "text       0\n",
      "tagid      0\n",
      "tag        0\n",
      "dtype: int64\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   tweetid  1000 non-null   int64 \n",
      " 1   text     1000 non-null   object\n",
      " 2   tagid    1000 non-null   int64 \n",
      " 3   tag      1000 non-null   object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 39.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#verify that it works.\n",
    "print(tweetDF.shape)\n",
    "print()\n",
    "print(tweetDF.isnull().sum())\n",
    "print()\n",
    "print(tweetDF.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets generate some summary statistics for our 1000 tweets that we imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269520/189792792.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  distDF[\"percentage\"][strIndex] = (distDF[\"sum\"][strIndex]*100)/totalSum\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SelfandID</th>\n",
       "      <td>607</td>\n",
       "      <td>60.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HealthySkepticism</th>\n",
       "      <td>319</td>\n",
       "      <td>31.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SocietalShift</th>\n",
       "      <td>49</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SearchTheVoid</th>\n",
       "      <td>25</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sum percentage\n",
       "SelfandID          607       60.7\n",
       "HealthySkepticism  319       31.9\n",
       "SocietalShift       49        4.9\n",
       "SearchTheVoid       25        2.5"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "distDF = pd.DataFrame(columns=[\"sum\",\"percentage\"],index=tweetDF.tag.unique())\n",
    "\n",
    "#thankfully, when we write the series to our col, it matches row index titles.\n",
    "distDF[\"sum\"] = tweetDF.groupby('tag')['tagid'].count()\n",
    "totalSum = tweetDF.shape[0]\n",
    "#distDF[\"percentage\"] = pd.Series([]) \n",
    "for strIndex in distDF.index:\n",
    "    distDF[\"percentage\"][strIndex] = (distDF[\"sum\"][strIndex]*100)/totalSum\n",
    "\n",
    "distDF\n",
    "#5% societal shift, 61% self, 2.5% searchthevoid, 32% Skepticism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our percentages above, we can see how much of our bandwidth is eaten up by specific tags. There probably isn't enough data for the SocietalShift and SearchTheVoid rules - these should be separated and ran over a longer time frame, from the other two rules.\n",
    "\n",
    "For the SelfandID, and HealthySkepticism matched tweets, we must separate poor tweets from good ones. This can be done as follows:\n",
    "\n",
    "1) A subset of tweets is visually inspected in VS Code (for ease). \"Good tweets are identified\".\n",
    "\n",
    "2) Good tweets are separated from bad ones.\n",
    "\n",
    "3) Next, word sets are accumulated for the Good and Bad tweet sets.\n",
    "\n",
    "4) We calculate the difference between the two sets. So unique words only found in the \"good\" tweets, and unique words only found in the \"bad\" tweets.\n",
    "\n",
    "5) From this information, we refine our rules and gather data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 4)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From our data table, lets separate our tweets by tag and write to file, to make tweet inspecting easier.\n",
    "\n",
    "#first get SelfandID tweets:\n",
    "\n",
    "selfDF =  tweetDF[tweetDF.tag.isin(['SelfandID'])]\n",
    "selfDF.shape\n",
    "\n",
    "skeptDF =  tweetDF[tweetDF.tag.isin(['HealthySkepticism'])]\n",
    "skeptDF.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can we write this to our data directory?\n",
    "\n",
    "selfDF.loc[:, ['tweetid', 'text']].to_csv(\"./data/self.twts\", sep='\\t', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeptDF.loc[:, ['tweetid', 'text']].to_csv(\"./data/skeptic.twts\", sep='\\t', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "cite2c": {
   "citations": {
    "8394687/9ZZ65S4Q": {
     "author": [
      {
       "family": "M. Cvetkovic",
       "given": "Dragos"
      },
      {
       "family": "Doob",
       "given": "Michael"
      },
      {
       "family": "Sachs",
       "given": "Horst"
      }
     ],
     "collection-title": "Pure and Applied Mathematics",
     "edition": "1st",
     "id": "8394687/9ZZ65S4Q",
     "issued": {
      "year": 1980
     },
     "language": "English",
     "number-of-pages": "368",
     "number-of-volumes": "1",
     "publisher": "Academic Press",
     "shortTitle": "Spectra of Graphs",
     "title": "Spectra of Graphs: Theory and Application",
     "title-short": "Spectra of Graphs",
     "type": "book",
     "volume": "87"
    },
    "8394687/EVEKS8I8": {
     "URL": "https://forum.bodybuilding.com/showthread.php?t=166912491&subId3=xid:fr1605396294772jac",
     "container-title": "Cocoa as weight-loss accelerator: Study",
     "genre": "Post",
     "id": "8394687/EVEKS8I8",
     "issued": {
      "day": 26,
      "month": 3,
      "year": 2015
     },
     "language": "English",
     "title": "Cocoa as weight-loss accelerator: Study",
     "type": "post"
    },
    "8394687/FB5G7ZR9": {
     "URL": "https://informationisbeautiful.net/visualizations/snake-oil-scientific-evidence-for-nutritional-supplements-vizsweet/",
     "author": [
      {
       "family": "David McCandless",
       "given": ""
      },
      {
       "family": "Dr Stephanie Starling",
       "given": ""
      },
      {
       "family": "et. al",
       "given": ""
      }
     ],
     "container-title": "Snake-Oil Supplements",
     "genre": "Infographic",
     "id": "8394687/FB5G7ZR9",
     "issued": {
      "day": 24,
      "month": 4,
      "year": 2019
     },
     "language": "English",
     "shortTitle": "Snake-Oil Supp.",
     "title": "Snake-Oil Supplements",
     "title-short": "Snake-Oil Supp.",
     "type": "webpage"
    },
    "8394687/SZS6I8KD": {
     "URL": "https://www.imdb.com/title/tt0119177/?ref_=fn_al_tt_1",
     "author": [
      {
       "family": "Niccol",
       "given": "Andrew"
      }
     ],
     "genre": "Sci-Fi, Drama",
     "id": "8394687/SZS6I8KD",
     "issued": {
      "day": 24,
      "month": 10,
      "year": 1997
     },
     "language": "English",
     "publisher": "Colombia Pictures",
     "shortTitle": "Gattaca",
     "title": "Gattaca",
     "title-short": "Gattaca",
     "type": "motion_picture"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
